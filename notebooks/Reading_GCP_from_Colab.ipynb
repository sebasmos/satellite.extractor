{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sebasmos/satellite.extractor/blob/main/Reading_GCP_from_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmdJgUdoTiuk",
    "outputId": "a2213cb1-ca8c-4a3c-9f84-8135a935df71"
   },
   "outputs": [],
   "source": [
    "!pip install epiweeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfQroQbldi2j"
   },
   "outputs": [],
   "source": [
    "def readImg(img_path, resize_ratio=None):\n",
    "  img = io.imread(img_path)\n",
    "\n",
    "  if resize_ratio:\n",
    "    img_rescale = rescale(img, resize_ratio, anti_aliasing=True)\n",
    "\n",
    "  print(os.path.basename(img_path), '(origin shape:', img.shape, '-> rescale:', str(img_rescale.shape) + ')')\n",
    "  return img_rescale\n",
    "\n",
    "\n",
    "# Load data from one of the source\n",
    "def loadData(csv_folder, img_folder, option=None, resize_ratio=None):\n",
    "  if option is None:\n",
    "    # Get data by combining from csv and images\n",
    "    df = loadStructuredData(csv_folder)\n",
    "    info_dict = combineData(img_folder, df, resize_ratio)\n",
    "    \n",
    "    print(len(info_dict['LastDayWeek']), len(info_dict['Image']), len(info_dict['cases_medellin']))\n",
    "\n",
    "  else:\n",
    "    # Load data from previous pickle file\n",
    "    info_dict = 1#loadDataFromPickle(option)\n",
    "  return info_dict\n",
    "  \n",
    "\n",
    "def loadStructuredData(csv_path):\n",
    "  df = pd.DataFrame()\n",
    "  if os.path.isdir(csv_path):\n",
    "    for filename in os.listdir(csv_path):\n",
    "      file_path = os.path.join(csv_path, filename)\n",
    "      df = df.append(pd.read_csv(file_path))\n",
    "  elif os.path.isfile(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "  else:\n",
    "    print('Error: Not folder or file')\n",
    "  return df\n",
    "  \n",
    "def getEpiWeek(origin_str):\n",
    "  \"\"\"Get epi week from string\n",
    "  \"\"\"\n",
    "  date_ls = origin_str.split('-')\n",
    "  return Week.fromdate(date(int(date_ls[0]), int(date_ls[1]), int(date_ls[2])))\n",
    "  \n",
    "def combineData(img_folder, df, resize_ratio=None):\n",
    "  info_dict = {'LastDayWeek':[], 'cases_medellin':[], 'Image':[], 'epi_week':[]}\n",
    "  img_list = os.listdir(img_folder)\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "    name = row['LastDayWeek']\n",
    "    week_df = str(getEpiWeek(name))\n",
    "    case = row['cases_medellin']\n",
    "    for img_name in img_list:\n",
    "      \n",
    "      # If image name is image_2017-12-24.tiff -> get 2017-12-24\n",
    "      # Reference Links: https://www.w3schools.com/python/ref_string_join.asp, \n",
    "      #                  https://stackoverflow.com/questions/13174468/how-do-you-join-all-items-in-a-list/13175535\n",
    "      new_img_name = ''.join(i for i in img_name if i.isdigit() or i == '-')      \n",
    "\n",
    "      week_img = str(getEpiWeek(new_img_name))\n",
    "      #print(f\"{week_df} = {week_img}\")\n",
    "      if week_df == week_img:\n",
    "        #print(\"ENTRO\")\n",
    "        img_path = os.path.join(img_folder, img_name)\n",
    "        img = readImg(img_path, resize_ratio)\n",
    "\n",
    "        info_dict['Image'].append(img)\n",
    "        info_dict['LastDayWeek'].append(name)\n",
    "        info_dict['cases_medellin'].append(case)\n",
    "        info_dict['epi_week'].append(week_df)\n",
    "        break\n",
    "\n",
    "  return info_dict\n",
    "\n",
    "def splitTrainTestSet(ratio):\n",
    "  # Split the data into training (ratio) and testing (1 - ratio)\n",
    "  train_val_ratio = ratio\n",
    "  train_num = int(len(info_dict['Image']) * train_val_ratio)\n",
    "\n",
    "  # Change list to array\n",
    "  origin_dimension_X = np.array(info_dict['Image'])\n",
    "  labels = np.array(info_dict['cases_medellin'])\n",
    "\n",
    "  print(''.center(60,'-'))\n",
    "\n",
    "  origin_X_train = origin_dimension_X[:train_num,:,:,:]\n",
    "  y_train = labels[:train_num]\n",
    "  origin_X_test = origin_dimension_X[train_num:,:,:,:]\n",
    "  y_test = labels[train_num:]\n",
    "\n",
    "  # print('Total number of weeks:'.ljust(30), len(origin_dimension_X), 'weeks')\n",
    "  # print('Training input:'.ljust(30), origin_X_train.shape)\n",
    "  # print('Training output:'.ljust(30), y_train.shape)\n",
    "  # print('Testing input:'.ljust(30), origin_X_test.shape)\n",
    "  # print('Testing output:'.ljust(30), y_test.shape) \n",
    "\n",
    "  # return origin_X_train, y_train, origin_X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "91lGjT5peHLf"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import  mean_absolute_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from random import randint, randrange\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "import skimage\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYqtUujJuBcS"
   },
   "source": [
    "# Download metadata:\n",
    "url = \"https://drive.google.com/u/0/uc?id=1RGrXHgvn60L4pHA40M0R0scszHLno5fD&export=download\"\n",
    "\n",
    "url = \"https://drive.google.com/file/d/1RGrXHgvn60L4pHA40M0R0scszHLno5fD/view?usp=sharing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mjWzOdNtEzE",
    "outputId": "3921ba18-18c5-4924-bd9e-6bc614d64352"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1RGrXHgvn60L4pHA40M0R0scszHLno5fD\n",
    "!unzip \"dengue.zip\" -d .\n",
    "!rm -f dengue.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU0NOUHgxEpw"
   },
   "source": [
    "## Mount GCP bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D78h8EZ2UFF-",
    "outputId": "bafc0866-201f-4742-cd09-6197e4d6eca0"
   },
   "outputs": [],
   "source": [
    "# authenticate\n",
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "\n",
    "# set your gcp project\n",
    "!gcloud config set project mit-hst-dengue\n",
    "\n",
    "!gsutil -q -m cp -r gs://colombia_sebasmos/DATASET_5_best_cities .\n",
    "\n",
    "!ls DATASET_5_best_cities/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXVpXCH2xPLc"
   },
   "source": [
    "## Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heWGi3bads5S",
    "outputId": "ecb226e9-63ba-4f7b-daaf-962543e0dac7"
   },
   "outputs": [],
   "source": [
    "csv_folder = \"/content/dengue/merge_cases_temperature_WeeklyPrecipitation_timeseries.csv\"\n",
    "\n",
    "img_folder = \"/content/DATASET_5_best_cities/MedelliÃÅn\"\n",
    "\n",
    "info_dict = loadData(csv_folder, img_folder, resize_ratio=(0.7, 0.7, 1))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Reading_GCP_from_Colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
