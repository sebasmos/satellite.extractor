{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sebasmos/satellite.extractor/blob/main/satellite_images_hashing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w73sDJbsTfxw"
   },
   "source": [
    "# Hash encrypting for satellite imagery \n",
    "\n",
    "The goal of hash encrypting is to create a unique fingerprint for each image per epiweek, so that even the change of color on a single pixel creates a different hash and thus to evaluate \"neighboring spatio-temporal images based on epiweek calendar\". For this purpose, image duplicate verification (i.e., checksums) is tested with sha-256, MD5 hash, and dhash(difference hash), obtaining best results for the last one using a hashsize of 8 pixels while squaring the horizontal gradients to obtain a lightweight hash per image. The last demonstrates to be useful to identify perceptually identical images, i.e, duplicate images captured during the data collection process with satellite extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EmdJgUdoTiuk",
    "outputId": "f69dc8e7-6e8c-4e6b-b5bf-aabe637ce391"
   },
   "outputs": [],
   "source": [
    "!pip install epiweeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vfQroQbldi2j"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import  mean_absolute_error\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from epiweeks import Week, Year\n",
    "from datetime import date\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "from random import randint, randrange\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "\n",
    "import skimage\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "!echo \"Libraries ready\"\n",
    "def readImg(img_path, resize_ratio=None, normalize = True):\n",
    "  img = io.imread(img_path)\n",
    "  img_F =img.copy()\n",
    "  if resize_ratio:\n",
    "    img_F = rescale(img, resize_ratio, anti_aliasing=True)\n",
    "  \n",
    "  path_img = os.path.basename(img_path)\n",
    "  if normalize:\n",
    "      CHANNELS = range(12)\n",
    "      img_F = np.dstack([\n",
    "          skimage.exposure.rescale_intensity(img_F[:,:,c], out_range=(0, 1)) \n",
    "          for c in CHANNELS])\n",
    "    \n",
    "  print(path_img, '(origin shape:', img.shape, '-> rescale:', str(img_F.shape) + ')')\n",
    "  return img_F, path_img\n",
    "\n",
    "  \n",
    "# Load data from one of the source\n",
    "def loadData(csv_folder, img_folder, option=None, resize_ratio=None):\n",
    "  if option is None:\n",
    "    # Get data by combining from csv and images\n",
    "    df = loadStructuredData(csv_folder)\n",
    "    info_dict = combineData(img_folder, df, resize_ratio)\n",
    "    \n",
    "    print(len(info_dict['LastDayWeek']), len(info_dict['Image']), len(info_dict['cases_medellin']))\n",
    "\n",
    "  else:\n",
    "    # Load data from previous pickle file\n",
    "    info_dict = 1#loadDataFromPickle(option)\n",
    "  return info_dict\n",
    "  \n",
    "\n",
    "def loadStructuredData(csv_path):\n",
    "  df = pd.DataFrame()\n",
    "  if os.path.isdir(csv_path):\n",
    "    for filename in os.listdir(csv_path):\n",
    "      file_path = os.path.join(csv_path, filename)\n",
    "      df = df.append(pd.read_csv(file_path))\n",
    "  elif os.path.isfile(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "  else:\n",
    "    print('Error: Not folder or file')\n",
    "  return df\n",
    "  \n",
    "def getEpiWeek(origin_str):\n",
    "  \"\"\"Get epi week from string\n",
    "  \"\"\"\n",
    "  date_ls = origin_str.split('-')\n",
    "  return Week.fromdate(date(int(date_ls[0]), int(date_ls[1]), int(date_ls[2])))\n",
    "  \n",
    "def combineData(img_folder, df, resize_ratio=None):\n",
    "  info_dict = {'LastDayWeek':[], 'cases_medellin':[], 'Image':[], 'epi_week':[], 'path_name':[]}\n",
    "  img_list = os.listdir(img_folder)\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "    name = row['LastDayWeek']\n",
    "    week_df = str(getEpiWeek(name))\n",
    "    case = row['cases_medellin']\n",
    "    for img_name in img_list:\n",
    "      \n",
    "      # If image name is image_2017-12-24.tiff -> get 2017-12-24\n",
    "      # Reference Links: https://www.w3schools.com/python/ref_string_join.asp, \n",
    "      #                  https://stackoverflow.com/questions/13174468/how-do-you-join-all-items-in-a-list/13175535\n",
    "      new_img_name = ''.join(i for i in img_name if i.isdigit() or i == '-')      \n",
    "\n",
    "      week_img = str(getEpiWeek(new_img_name))\n",
    "      #print(f\"{week_df} = {week_img}\")\n",
    "      if week_df == week_img:\n",
    "        #print(\"ENTRO\")\n",
    "        img_path = os.path.join(img_folder, img_name)\n",
    "        img, img_path = readImg(img_path, resize_ratio)\n",
    "        print(f\"path {img_path} : {img.dtype} - {np.unique(img)}\")\n",
    "        info_dict['Image'].append(np.array(img).astype(\"float32\"))\n",
    "        info_dict['LastDayWeek'].append(name)\n",
    "        info_dict['cases_medellin'].append(case)\n",
    "        info_dict['epi_week'].append(week_df)\n",
    "        info_dict['path_name'].append(img_path)\n",
    "        break\n",
    "\n",
    "  return info_dict\n",
    "\n",
    "def splitTrainTestSet(ratio):\n",
    "  # Split the data into training (ratio) and testing (1 - ratio)\n",
    "  train_val_ratio = ratio\n",
    "  train_num = int(len(info_dict['Image']) * train_val_ratio)\n",
    "\n",
    "  # Change list to array\n",
    "  origin_dimension_X = np.array(info_dict['Image'])\n",
    "  labels = np.array(info_dict['cases_medellin'])\n",
    "\n",
    "  print(''.center(60,'-'))\n",
    "\n",
    "  origin_X_train = origin_dimension_X[:train_num,:,:,:]\n",
    "  y_train = labels[:train_num]\n",
    "  origin_X_test = origin_dimension_X[train_num:,:,:,:]\n",
    "  y_test = labels[train_num:]\n",
    "\n",
    "  # print('Total number of weeks:'.ljust(30), len(origin_dimension_X), 'weeks')\n",
    "  # print('Training input:'.ljust(30), origin_X_train.shape)\n",
    "  # print('Training output:'.ljust(30), y_train.shape)\n",
    "  # print('Testing input:'.ljust(30), origin_X_test.shape)\n",
    "  # print('Testing output:'.ljust(30), y_test.shape) \n",
    "\n",
    "  # return origin_X_train, y_train, origin_X_test, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYqtUujJuBcS"
   },
   "source": [
    "### Download metadata:\n",
    "url = \"https://drive.google.com/u/0/uc?id=1RGrXHgvn60L4pHA40M0R0scszHLno5fD&export=download\"\n",
    "\n",
    "url = \"https://drive.google.com/file/d/1RGrXHgvn60L4pHA40M0R0scszHLno5fD/view?usp=sharing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mjWzOdNtEzE",
    "outputId": "50bc1836-5b5a-4e63-d3c3-d6b0daa88aa6"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!gdown --id 1RGrXHgvn60L4pHA40M0R0scszHLno5fD\n",
    "!unzip \"dengue.zip\" -d .\n",
    "!rm -f dengue.zip\n",
    "\"\"\"\n",
    "!gdown --id 1x-A6uc91FS3HePSy11vxGJjP0qMqyhyF\n",
    "#!unzip \"dengue.zip\" -d .\n",
    "#!rm -f dengue.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oci\n",
    "import sys\n",
    "import json\n",
    "import os\n",
    "import skimage\n",
    "from tifffile import imread, imwrite\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Storage Parameters (for building a connection)\n",
    "os_namespace = 'idfgbkbot6fd'\n",
    "os_bucket = 'Sentinel_2_Images'\n",
    "os_cmpt = 'ocid1.compartment.oc1..aaaaaaaamgl2tu5vh7mcyjsd2h7tq5umvcq6ivcjftljmzaq53kct6khrd7q'\n",
    "os_tenancy = 'ocid1.tenancy.oc1..aaaaaaaas5kw6lnv6emziaapx3zdew6pqqd4xsnsovdlft3is6dy6skkg57q'\n",
    "\n",
    "# Resource Principal for connecting to Object Storage/Logs\n",
    "signer = oci.auth.signers.get_resource_principals_signer()\n",
    "client = oci.object_storage.ObjectStorageClient(config={}, signer=signer, timeout=(100, 100))\n",
    "objects = client.list_objects(namespace_name=os_namespace,\n",
    "                        bucket_name=os_bucket)\n",
    "destination = \"/home/datascience\"\n",
    "OUTPUT = \"DATASET_5_best_cities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_bucket(OUTPUT, \n",
    "                   os_namespace,\n",
    "                   os_bucket,\n",
    "                   objects):\n",
    "    i = 0\n",
    "    for obj in objects.data.objects:\n",
    "        if OUTPUT in obj.name and (\"tiff\" in obj.name):\n",
    "            #print(f\"Downloading image: {obj.name}\")\n",
    "            image = client.get_object(namespace_name=os_namespace,\n",
    "                                bucket_name=os_bucket,\n",
    "                                object_name=obj.name)\n",
    "            local_path = os.path.join(destination, OUTPUT, obj.name.split('/')[1])\n",
    "            if not os.path.exists(local_path):\n",
    "                os.makedirs(local_path)\n",
    "\n",
    "            image_bytes = io.BytesIO(image.data.content)\n",
    "\n",
    "            filename = obj.name.split('/')[1]\n",
    "\n",
    "            with open(os.path.join(destination, obj.name), \"wb\", buffering=0) as f:\n",
    "                f.write(image.data.content)\n",
    "            i+=1\n",
    "    return print(\"Finished\")\n",
    "\n",
    "download_bucket(OUTPUT, \n",
    "                   os_namespace,\n",
    "                   os_bucket,\n",
    "                   objects\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXVpXCH2xPLc"
   },
   "source": [
    "### Load dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heWGi3bads5S",
    "outputId": "4473a821-d9f5-470a-c591-67e410f0fe00"
   },
   "outputs": [],
   "source": [
    "#csv_folder # \"./data/merge_cases_temperature_WeeklyPrecipitation_timeseries.csv\"\n",
    "csv_folder = \"/home/sebasmos/Desktop/satellite.extractor/data/merge_cases_temperature_WeeklyPrecipitation_timeseries.csv\"\n",
    "#img_folder = \"/home/datascience/DATASET_5_best_cities/Medellín\"\n",
    "#img_folder = \"/home/sebasmos/Desktop/DATASETS/DATASET_5_best_cities/Cali\"\n",
    "img_folder = \"/home/sebasmos/Desktop/DATASETS/Dataset_10_best_cities/Dataset_10_best_cities/5001\"\n",
    "\n",
    "info_dict = loadData(csv_folder, img_folder, resize_ratio=(0.1, 0.1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RIXcJ6dFDe7I",
    "outputId": "289e1b49-fb70-4a00-dc22-617e13b2ed6e"
   },
   "outputs": [],
   "source": [
    "# Change list to array\n",
    "df_med = np.array(info_dict['Image'])\n",
    "labels = np.array(info_dict['cases_medellin'])\n",
    "img_names = np.array(info_dict['path_name'])\n",
    "\n",
    "print(\"Checking errors: \",[print(x.dtype) for x in  info_dict['Image'] if x.dtype!=np.float32])\n",
    "print(''.center(60,'-'))\n",
    "print(f\"shape origin_dimension_X: {df_med.shape}\")\n",
    "print(f\"labels {labels.shape}\")\n",
    "#print(f\"labels {paths.shape}\")\n",
    "# Filter channels\n",
    "df_med_rgb = df_med[:,:,:,1:4]\n",
    "df_med_rgb = np.array(df_med_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D7u2rmCuIRWy",
    "outputId": "60c1029f-bf39-4168-841d-f21a87706e05"
   },
   "outputs": [],
   "source": [
    "N_SIZE = len(img_names)\n",
    "df_test = df_med_rgb[:N_SIZE]\n",
    "path_test = img_names[:N_SIZE]\n",
    "print(df_test.shape)\n",
    "print(path_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8hMaluJGKkK"
   },
   "source": [
    "### Similarity analysis based on [hash imag](https://github.com/JohannesBuchner/imagehash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7SLIaDJFaN9"
   },
   "outputs": [],
   "source": [
    "#!pip install ImageHash\n",
    "#! pip install imutils\n",
    "import cv2 \n",
    "from PIL import Image\n",
    "#import imagehash\n",
    "from imutils import paths\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "import cv2\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifuBpvVtH7k2",
    "outputId": "1b067d81-b103-4c0a-a90d-0daaa350e893"
   },
   "outputs": [],
   "source": [
    "def hash_difference(image, hashSize=8):\n",
    "\tresized = cv2.resize(image, (hashSize + 1, hashSize))\n",
    "\tdiff = resized[:, 1:] > resized[:, :-1]\n",
    "\treturn sum([2 ** i for (i, v) in enumerate(diff.flatten()) if v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBLmumjoNng0"
   },
   "outputs": [],
   "source": [
    "dict_hash = []\n",
    "for img in df_test:\n",
    "  img_float32 = np.float32(img)\n",
    "  image = cv2.cvtColor(img_float32, cv2.COLOR_BGR2RGB)\n",
    "  imageHash = hash_difference(image)\n",
    "  dict_hash.append(imageHash)\n",
    "\n",
    "data = {\"img_hash\": dict_hash,\n",
    "        \"img_names\": path_test }\n",
    "df = pd.DataFrame(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "oA3qtV9xUMN5",
    "outputId": "6d413616-b9f3-4521-ec8e-d148f11f316e"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "Mkzrb-n1QZEv",
    "outputId": "f843d154-9086-4151-c0b9-ce057b4fe61c"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"img_hash\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "ASaj_1EQVcZn",
    "outputId": "78526b03-eb72-4c0f-beb7-e37d453e5b63"
   },
   "outputs": [],
   "source": [
    "num_img_per_hash = df.groupby(\"img_hash\").size().sort_values(ascending = False).reset_index(name = \"img_names\")\n",
    "display(num_img_per_hash.head(10))\n",
    "print(sum(num_img_per_hash[\"img_names\"]))\n",
    "num_img_per_hash.hist(\"img_names\", bins = 20)\n",
    "plt.xlabel('# Repeated hashes')\n",
    "plt.title('hashes per img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYDrGaxnWkvy",
    "outputId": "84ad5d79-e28b-48ec-de0e-1a4d1dd6f80d"
   },
   "outputs": [],
   "source": [
    "num_img_per_hash.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting *vertical* perspective: Frequency vs # images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img_per_hash = df.groupby(\"img_hash\").size().sort_values(ascending = False).reset_index(name = \"img_names\")\n",
    "display(num_img_per_hash.head(10))\n",
    "print(sum(num_img_per_hash[\"img_names\"]))\n",
    "\n",
    "def bins_labels(bins, **kwargs):\n",
    "    bin_w = (max(bins) - min(bins)) / (len(bins) - 1)\n",
    "    plt.xticks(np.arange(min(bins)+bin_w/2, max(bins), bin_w), bins, **kwargs)\n",
    "    plt.xlim(bins[0], bins[-1])\n",
    "ax = plt.figure()\n",
    "num_img_per_hash.hist(\"img_names\", \n",
    "                        bins = range(20), \n",
    "                        orientation = \"horizontal\", \n",
    "                        color='#86bf91',  \n",
    "                        edgecolor='black', \n",
    "                        linewidth=1.2, \n",
    "                        align=\"left\",\n",
    "                        rwidth=0.85,\n",
    "                        alpha=0.75)\n",
    "plt.xlabel(\"Number of Encrypted images\")\n",
    "plt.ylabel(\"Relative Frequency\")\n",
    "plt.title('')\n",
    "plt.ylim(0, num_img_per_hash.img_names[0]+0.5)\n",
    "plt.grid(False)\n",
    "\n",
    "# ------- \n",
    "# make the y ticks integers, not floats\n",
    "yint = []\n",
    "locs, labels = plt.yticks()\n",
    "for each in locs:\n",
    "    yint.append(int(each))\n",
    "plt.yticks(yint)\n",
    "# -------\n",
    "plt.savefig('plot2.png', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    a = 0\n",
    "    root = config.DATASET\n",
    "    for city in os.listdir(root):\n",
    "        start_time = time.time()\n",
    "        list_images = []\n",
    "        images_path = os.path.join(root, city)\n",
    "        print(f'{city} : {len(os.listdir(images_path))} images '.center(60,\"-\"))\n",
    "        for img_path in os.listdir(images_path):\n",
    "            path = os.path.join(root, city, img_path)\n",
    "            print(\"Processing image \", os.path.join(root, city, img_path))\n",
    "            img = utils.read_tiff(path, config.image_size, resize_ratio=config.resize_ratio, resizing = config.resizing, normalize=config.normalize, printing=config.printing)\n",
    "            list_images.append(img)\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print(f'Training time for {city} : {total_time_str}')\n",
    "        m = np.array(list_images)\n",
    "        print(np.array(list_images).shape)\n",
    "        dict_hash = []\n",
    "        for img in m:\n",
    "            imageHash = utils.hash_difference(img)\n",
    "            dict_hash.append(imageHash)\n",
    "        data = {\"img_hash\": dict_hash,\"img_names\": os.listdir(images_path) }\n",
    "        df = pd.DataFrame(data = data)\n",
    "        num_img_per_hash = df.groupby(\"img_hash\").size().sort_values(ascending = False).reset_index(name = \"img_names\")\n",
    "        print(num_img_per_hash)\n",
    "        #display(num_img_per_hash.head(10))\n",
    "        print(sum(num_img_per_hash[\"img_names\"]))\n",
    "        ax = plt.figure()\n",
    "        num_img_per_hash.hist(\"img_names\", bins = 20, orientation = \"vertical\",  color='#86bf91',  edgecolor='black', linewidth=1.2)\n",
    "        plt.xlabel(\"Relative Frequency\")\n",
    "        plt.ylabel(\"Number of Encrypted images\")\n",
    "        plt.title('')\n",
    "        plt.xlim(1, num_img_per_hash.img_names[0])\n",
    "        plt.grid(False)\n",
    "\n",
    "        # ------- \n",
    "        # make the y ticks integers, not floats\n",
    "        xint = []\n",
    "        locs, labels = plt.xticks()\n",
    "        for each in locs:\n",
    "            xint.append(int(each))\n",
    "        plt.xticks(xint)\n",
    "        # -------\n",
    "        plt.savefig(f'{viz}/{city}.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting *horizontal* perspective: Frequency vs # images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img_per_hash = df.groupby(\"img_hash\").size().sort_values(ascending = False).reset_index(name = \"img_names\")\n",
    "display(num_img_per_hash.head(10))\n",
    "print(sum(num_img_per_hash[\"img_names\"]))\n",
    "\n",
    "num_img_per_hash.img_names = num_img_per_hash.img_names - 1 # Axis starts from 0\n",
    "\n",
    "ax = plt.figure()\n",
    "num_img_per_hash.hist(\"img_names\", \n",
    "                        bins = range(20), \n",
    "                        orientation = \"vertical\", \n",
    "                        color='#FF0000',  \n",
    "                        edgecolor='black', \n",
    "                        #linewidth=1.2, \n",
    "                        align=\"left\",\n",
    "                        rwidth=0.85,\n",
    "                        alpha=0.5)\n",
    "plt.xlabel(\"Number of times an image is repeated\")\n",
    "plt.ylabel(\"Number of Encrypted images\")\n",
    "plt.title('')                                       # Delete title\n",
    "plt.xlim(-0.5, num_img_per_hash.img_names[0]+0.5)   # Starting axis from zero with 0.5 padding side-wise\n",
    "plt.grid(False)\n",
    "print(num_img_per_hash.img_names[0])\n",
    "# Design x axis to be integer and avoid float metrics\n",
    "xint = []\n",
    "locs, labels = plt.xticks()\n",
    "for each in locs:\n",
    "    xint.append(int(each))\n",
    "plt.xticks(xint)\n",
    "# Store image\n",
    "plt.savefig('plot2.png', transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_img_per_hash.img_names[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improved hashing for N cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/home/sebasmos/Desktop/satellite.extractor/notebooks\" \n",
    "a = 0\n",
    "# \"/home/sebasmos/Desktop/DATASETS/test\"\n",
    "root = \"/home/sebasmos/Desktop/DATASETS/Dataset_10_best_cities/Dataset_10_best_cities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import copy\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage.transform import rescale, resize\n",
    "import skimage.exposure\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from imutils import paths\n",
    "import argparse\n",
    "import time\n",
    "import sys\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "viz = os.path.join(ROOT_DIR, \"data\")\n",
    "os.makedirs(viz, exist_ok=True)\n",
    "\n",
    "def hash_difference(image, hashSize=8):\n",
    "\tresized = cv2.resize(image, (hashSize + 1, hashSize))\n",
    "\tdiff = resized[:, 1:] > resized[:, :-1]\n",
    "\treturn sum([2 ** i for (i, v) in enumerate(diff.flatten()) if v])\n",
    "\n",
    "def read_tiff(img_path, image_size, resize_ratio=None, resizing = True, normalize=True, printing=True):\n",
    "  img = io.imread(img_path)\n",
    "  img_F =img.copy()\n",
    "  if resize_ratio:\n",
    "    img_F = rescale(img, resize_ratio, anti_aliasing=True)\n",
    "  if resizing:\n",
    "    img_F = resize(img_F, (image_size, image_size), anti_aliasing=True)\n",
    "  \n",
    "  path_img = os.path.basename(img_path)\n",
    "  if normalize:\n",
    "    CHANNELS = range(12)\n",
    "    img_F = np.dstack([\n",
    "        skimage.exposure.rescale_intensity(img_F[:,:,c], out_range=(0, 1)) \n",
    "        for c in CHANNELS])\n",
    "  if printing:\n",
    "    print(f\"(origin shape: {path_img}: {img.shape} -> rescale: {str(img_F.shape)}) - Range -> [{img_F.min(), img_F.max()}]\")\n",
    "  return img_F\n",
    "  \n",
    "for city in os.listdir(root):\n",
    "        start_time = time.time()\n",
    "        list_images = []\n",
    "        images_path = os.path.join(root, city)\n",
    "        print(f'{city} : {len(os.listdir(images_path))} images '.center(60,\"-\"))\n",
    "        for img_path in os.listdir(images_path):\n",
    "            path = os.path.join(root, city, img_path)\n",
    "            #print(\"Processing image \", os.path.join(root, city, img_path))\n",
    "            img = read_tiff(path, 750, resize_ratio=(0.1,0.1,1), resizing = False, normalize=False, printing=True)\n",
    "            list_images.append(img)\n",
    "        m = np.array(list_images)\n",
    "        print(np.array(list_images).shape)\n",
    "        dict_hash = []\n",
    "        for img in m:\n",
    "            imageHash = hash_difference(img)\n",
    "            dict_hash.append(imageHash)\n",
    "        data = {\"img_hash\": dict_hash,\"img_names\": os.listdir(images_path) }\n",
    "        df = pd.DataFrame(data = data)\n",
    "        num_img_per_hash = df.groupby(\"img_hash\").size().sort_values(ascending = False).reset_index(name = \"img_names\")\n",
    "        num_img_per_hash.img_names = num_img_per_hash.img_names - 1 # Axis starts from 0\n",
    "        ax = plt.figure()\n",
    "        if num_img_per_hash.img_names[0]<=13:\n",
    "          num_img_per_hash.hist(\"img_names\", \n",
    "                                bins = range(20), \n",
    "                                orientation = \"vertical\",  \n",
    "                                color='#FF0000',  \n",
    "                                edgecolor='black', \n",
    "                                linewidth=1.2,\n",
    "                                rwidth=0.85,\n",
    "                                alpha=0.5,\n",
    "                                align=\"left\")\n",
    "        \n",
    "        else:\n",
    "          num_img_per_hash.hist(\"img_names\", \n",
    "                                bins = 20, \n",
    "                                orientation = \"vertical\",  \n",
    "                                color='#FF0000',  \n",
    "                                edgecolor='black', \n",
    "                                linewidth=1.2,\n",
    "                                alpha=0.5,\n",
    "                                align=\"mid\")\n",
    "        \n",
    "        plt.xlabel(\"Number of times an image is repeated\")\n",
    "        plt.ylabel(\"Number of Encrypted images\")\n",
    "        plt.title('')\n",
    "        plt.xlim(-1, num_img_per_hash.img_names[0]+0.5)\n",
    "        plt.grid(False)\n",
    "\n",
    "        # ------- \n",
    "        # make the y ticks integers, not floats\n",
    "        xint = []\n",
    "        locs, labels = plt.xticks()\n",
    "        for each in locs:\n",
    "            xint.append(int(each))\n",
    "        plt.xticks(xint)\n",
    "        # -------\n",
    "        plt.savefig(f'{viz}/{city}.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure()\n",
    "num_img_per_hash.hist(\"img_names\", \n",
    "                      bins = 20, orientation = \"vertical\",  color='#FF0000',  \n",
    "                      edgecolor='black', \n",
    "                      linewidth=2,\n",
    "                      rwidth=2, \n",
    "                      alpha=0.5)\n",
    "plt.xlabel(\"Relative Frequency\")\n",
    "plt.ylabel(\"Number of Encrypted images\")\n",
    "plt.title('')\n",
    "plt.xlim(1, num_img_per_hash.img_names[0])\n",
    "plt.grid(False)\n",
    "xint = []\n",
    "locs, labels = plt.xticks()\n",
    "for each in locs:\n",
    "            xint.append(int(each))\n",
    "plt.xticks(xint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lovUfk4YWfQq"
   },
   "source": [
    "* **Remarks on image hashing for high-cloud inteference**\n",
    "\n",
    "The data used corresponds to Dengue Satellite data of Colombia, extracted for MIT critical Data Colombia, in open-source collaboration with the University of Bordeaux, Autonomous University of Madrid, and Pázmány Péter Catholic University. \n",
    "\n",
    "[Satellite-collector](https://github.com/sebasmos/satellite.extractor/blob/main/linux/request_multiple_images_by_slots.py) algorithm selects the best image per week when there is an image with the least interference during the same week; if otherwise - when all images are totally occluded due to cloud interference, an extra day from the following week is added into the epiweek consecutively, until a better image than the ones within this pool is captured.\n",
    "\n",
    "Despite the mosaicking order is based on the LeastCC algorithm, the fact that there can be cloud-occluded images during months means that there can be images that can be repeated a certain number of times within the downloaded dataset (around a month would be reasonable in most places, but this would depend of the local ecosystems and satellite measurement conditions). As examplified by the worst-case scenario, when every day it is totally covered by clouds, we expect to encounter cases when a very same image (with the same hash) might be repeated several times, e.g. in the case of some tropical areas like Amazonas (one of the most tropical places on earth), it can be cloudy up to 4 consecutive months from December to April, meaning that an image could be repeated up to 16 times in a row. \n",
    "\n",
    "The most effective way to analyse this periodic behaviour is using hashes, which maps a data structure of arbitrary size to fixed-sized data. In our case, we use the hashes to create a hash-image dictionary and compare how often the images repeat over the entire dataset tagged by the name correspoding to the image. On the case above, we counted with the satellite data extracted from [satellite extractor](https://github.com/sebasmos/satellite.extractor), which contains 146 images from the city of Medellin and from which we observed that two images repeated a maximum number of three times, meaning that this city was totally occluded by clouds during a month. In order to download the images from sentinel-hub API, a script request is configured using the least amount of clouds per epi week where the satellite image with least amount of clouds is selected to create a curated dataset.  *However this is not common, as it can be seen that there are duplicated images for less than 18 images out of 146, whilst all the other images preserve a unique hash, meaning that each image is unique in resolution and is different from each other in 80 % of the time for the city of Medellin*. A similar analysis can be performed for any dataset, from any city and from any type of dataset.  \n",
    "\n",
    "\n",
    "From this analysis, we concluded that the images we downloaded are each unique and can be used for change-detection analysis and dengue forecasting, as there are geographical places where rainy days can last for months and therefore it is natural to encounter totally occluded images during certain months, however this can be represented in less than 15 % of the data, meaning that more pre-processing could be performed to deal with the cloud interference problem. Besides, vector-borne diseases are highly correlated with coulds as they relate to temperature, so there might be a relationship between mosquito migration sparsity and the 15% of cloud interference on this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "https://realpython.com/python-histograms/ "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "satellite_images_hashing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sebasmos')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad1edc83c8a3ba973d0a5ed89f8d56b42dac8fe366158fcfa7dfa82a998d469f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
